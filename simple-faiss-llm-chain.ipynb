{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/dhorton/anaconda3/lib/python3.11/site-packages (1.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-openai in /Users/dhorton/anaconda3/lib/python3.11/site-packages (0.0.7)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.26 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.1.26)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.26.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-openai) (1.10.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-openai) (0.5.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (4.2.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (0.1.7)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.6.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.26->langchain-openai) (8.2.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.10.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.26->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.26->langchain-openai) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.26->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.26->langchain-openai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.26->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dhorton/anaconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.26->langchain-openai) (1.26.18)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement python-magic-bin (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for python-magic-bin\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "%pip install -U langchain-openai\n",
    "%pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from getpass import getpass\n",
    "import nltk\n",
    "\n",
    "openai_api_key = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "root_folder = '/Users/dhorton/beachdog-enterprises/beachdog-networks/git/jambones.org/git/hosted-cpaas/next-static-site/markdown/docs/webhooks'\n",
    "loader = DirectoryLoader(root_folder, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docsearch = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up your LLM\n",
    "llm = OpenAI(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your Retriever\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"map_reduce\", retriever=docsearch.as_retriever(), \n",
    "                                 return_source_documents=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'Does jambonz support IBM as a speech recognizer?  Answer only from the context documents provided', 'result': ' No, jambonz does not support IBM as a speech recognizer.', 'source_documents': [Document(page_content='recognizer\\n\\nThe recognizer property is used in multiple verbs (gather, transcribe, dial). It selects and configures the speech recognizer.\\n\\nIt is an object containing the following properties:\\n\\noption description required vendor Speech vendor to use (see list below, along with any others you add via the custom speech API ) no language Language code to use for speech detection.  Defaults to the application level setting no interim If true, interim transcriptions are sent no (default: false) hints (google, microsoft, deepgram, nvidia, soniox) Array of words or phrases to assist speech detection.  See examples below. no hintsBoost (google, nvidia) Number indicating the strength to assign to the configured hints.  See examples below. no profanityFilter (google, deepgram, nuance, nvidia) If true, filter profanity from speech transcription .  Default:  no no singleUtterance (google) If true, return only a single utterance/transcript no (default: true for gather) vad.enable If true, delay connecting to cloud recognizer until speech is detected no vad.voiceMs If vad is enabled, the number of milliseconds of speech required before connecting to cloud recognizer no vad.mode If vad is enabled, this setting governs the sensitivity of the voice activity detector; value must be between 0 to 3 inclusive, lower numbers mean more sensitive no separateRecognitionPerChannel If true, recognize both caller and called party speech using separate recognition sessions no altLanguages (google, microsoft) An array of alternative languages that the speaker may be using no punctuation (google) Enable automatic punctuation no model (google) speech recognition model to use no (default: phone_call) enhancedModel (google) Use enhanced model no words (google) Enable word offsets no diarization (google) Enable speaker diarization no diarizationMinSpeakers (google) Set the minimum speaker count no diarizationMaxSpeakers (google) Set the maximum speaker count no interactionType (google) Set the interaction type: discussion, presentation, phone_call, voicemail, professionally_produced, voice_search, voice_command, dictation no naicsCode (google) set an industry NAICS code that is relevant to the speech no vocabularyName (aws) The name of a vocabulary to use when processing the speech. no vocabularyFilterName (aws) The name of a vocabulary filter to use when processing the speech. no filterMethod (aws) The method to use when filtering the speech: remove, mask, or tag. no identifyChannels (aws) Enable channel identification. no profanityOption (microsoft) masked, removed, or raw.  Default:  raw no outputFormat (microsoft) simple or detailed.  Default:  simple no requestSnr (microsoft) Request signal to noise information no initialSpeechTimeoutMs (microsoft) Initial speech timeout in milliseconds no transcriptionHook Webhook to receive an HTPP POST when an interim or final transcription is received. yes asrTimeout timeout value for continuous ASR feature no asrDtmfTerminationDigit DMTF key that terminates continuous ASR feature no azureServiceEndpoint Custom service endpoint to connect to, instead of hosted Microsoft regional endpoints no azureOptions (added in 0.8.5) Azure-specific speech recognition options (see below) no deepgramOptions (added in 0.8.0) Deepgram-specific speech recognition options (see below) no ibmOptions (added in 0.8.0) IBM Watson-specific speech recognition options (see below) no nuanceOptions (added in 0.8.0) Nuance-specific speech recognition options (see below) no nvidiaOptions (added in 0.8.0) Nvidia-specific speech recognition options (see below) no sonioxOptions (added in 0.8.2) Soniox-specific speech recognition options (see below) no\\n\\nSpeech-to-text vendors\\n\\njambonz natively supports the following speech-to-text services:\\n- assemblyai\\n- aws\\n- azure\\n- cobalt\\n- deepgram\\n- google\\n- ibm\\n- nuance\\n- nvidia\\n- sonoix\\n\\nProviding speech hints\\n\\ngoogle, microsoft, deepgram, and nvidia all support the ability to provide a dynamic list of words or phrases that should be \"boosted\" by the recognizer, i.e. the recognizer should be more likely to detect this terms and return them in the transcript.  A boost factor can also be applied.  In the most basic implementation it would look like this:\\n\\njson\\n\"hints\": [\"benign\", \"malignant\", \"biopsy\"],\\n\"hintsBoost\": 50\\n\\nAdditionally, google and nvidia allow a boost factor to be specified at the phrase level, e.g.\\n\\njson\\n\"hints\": [\\n  {\"phrase\": \"benign\", \"boost\": 50},\\n  {\"phrase\": \"malignant\", \"boost\": 10},\\n  {\"phrase\": \"biopsy\", \"boost\": 20},\\n]\\n\\nazureOptions\\n\\nazureOptions is an object with the following properties. This option is available in jambonz 0.8.5 or above.\\n\\noption description required speechSegmentationSilenceTimeoutMs Duration (in ms) of nonspeech audio within a phrase that\\'s currently being spoken before that phrase is considered \"done.\" See here for details no\\n\\nnuanceOptions\\n\\nnuanceOptions is an object with the following properties. Please refer to the Nuance Documentation for detailed descriptions.  This option is available in jambonz 0.8.0 or above.\\n\\noption description required clientId Nuance client ID to authenticate with (overrides setting in jambonz portal) no secret Nuance secret to authenticate with (overrides setting in jambonz portal) no kryptonEndpoint Endpoint of on-prem Krypton endpoint to connect to no (defaults to hosted service) topic specialized language model no utteranceDetectionMode How many sentences (utterances) within the audio stream are processed (\\'single\\', \\'multiple\\', \\'disabled\\') no (default: single punctuation Whether to enable auto punctuation no includeTokenization Whether to include tokenized recognition result. no discardSpeakerAdaptation If speaker profiles are used, whether to discard updated speaker data. By default, data is stored. no suppressCallRecording Whether to disable call logging and audio capture. By default, call logs, audio, and metadata are collected. no maskLoadFailures whether to terminate recogition when failing to load external resources no suppressInitialCapitalization When true, the first word in a sentence is not automatically capitalized. no allowZeroBaseLmWeight When true, custom resources (DLMs, wordsets, etc.) can use the entire no filterWakeupWord Whether to remove the wakeup word from the final result. no resultType The level of recognition results (\\'final\\', \\'partial\\', \\'immutable_partial\\') no (default: final) noInputTimeoutMs Maximum silence, in milliseconds, allowed while waiting for user input after recognition timers are started. no recognitionTimeoutMs Maximum duration, in milliseconds, of recognition turn no utteranceEndSilenceMs Minimum silence, in milliseconds, that determines the end of a sentence no maxHypotheses Maximum number of n-best hypotheses to return no speechDomain Mapping to internal weight sets for language models in the data pack no userId Identifies a specific user within the application no speechDetectionSensitivity A balance between detecting speech and noise (breathing, etc.), 0 to 1. 0 means ignore all noise, 1 means interpret all noise as speech no (default: 0.5) clientData An object containing arbitrary key, value pairs to inject into the call log. no formatting.scheme Keyword for a formatting type defined in the data pack no formatting.options Object containing key, value pairs of formatting options and values defined in the data pack no resource An array of zero or more recognition resources (domain LMs, wordsets, etc.) to improve recognition no resource[].inlineWordset Inline wordset JSON resource. See Wordsets for details no resource[].builtin Name of a builtin resource in the data pack no resource[].inlineGrammar Inline grammar, SRGS XML format no resource[].wakeupWord Array of wakeup words no resource[].weightName input field setting the weight of the domain LM or builtin relative to the data pack (\\'defaultWeight\\', \\'lowest\\', \\'low\\', \\'medium\\', \\'high\\', \\'highest\\') no (default = MEDIUM resource[].weightValue Weight of DLM or builtin as a numeric value from 0 to 1 no (default: 0.25) resource[].reuse Whether the resource will be used multiple times (\\'undefined_reuse\\', \\'low_reuse\\',\\'high_reuse\\') no (default: low_reuse resource[].externalReference An external DLM or settings file for creating or updating a speaker profile no resource[].externalReference.type Resource type (\\'undefined_resource_type\\', \\'wordset\\', \\'compiled_wordset\\', \\'domain_lm\\', \\'speaker_profile\\', \\'grammar\\', \\'settings\\') no resource[].externalReference.uri Location of the resource as a URN reference no resource[].externalReference.maxLoadFailures when true allow transcription to proceed resource loading fails no resource[].externalReference.requestTimeoutMs Time to wait when downloading resources no resource[].externalReference.headers An object containing HTTP cache-control directives (e.g. max-age etc) no\\n\\ndeepgramOptions\\n\\ndeepgramOptions is an object with the following properties. Please refer to the Deepgram Documentation for detailed descriptions. This option is available in jambonz 0.8.0 or above.\\n\\noption description required apiKey Deepgram api key to authenticate with (overrides setting in jambonz portal) no tier Deepgram tier you would like to use (\\'enhanced\\', \\'base\\') no (default: base) model Deepgram model used to process submitted audio (\\'general\\', \\'meeting\\', \\'phonecall\\', \\'voicemail\\', \\'finance\\', \\'conversationalai\\', \\'video\\', \\'custom\\') no (default: general) endpointing Indicates the number of milliseconds of silence Deepgram will use to determine a speaker has finished saying a word or phrase. The value provided must be iether a number of milliseconds or \\'false\\' to disable the feature entirely.  Note: the default endpointing value that Deepgram uses is 10 milliseconds.  You can set this value higher to allow to require more silence before a final transcript is returned but we suggest a value of 1000 (one second) or less, as we have observed strange behaviors with higher values.  If you wish to allow more time for pauses during a conversation before returning a transcript, we suggest using the utteranceEndMs feature instead that is described below. no (default: 10ms) customModel Id of custom model no version Deepgram version of model to use no (default: latest) punctuate Indicates whether to add punctuation and capitalization to the transcript no profanityFilter Indicates whether to remove profanity from the transcript no redact Whether to redact information from transcripts (\\'pci\\', \\'numbers\\', \\'true\\', \\'ssn\\') no diarize Wehther to assign a speaker to each word in the transcript no diarizeVersion if set to \\'2021-07-14.0\\' the legacy diarization feature will be used no multichannel Indicates whether to transcribe each audio channel independently no alternatives Number of alternative transcripts to return no numerals Indicates whether to convert numbers from written format (e.g., one) to numerical format (e.g., 1) no search An array of terms or phrases to search for in the submitted audio no replace An array of terms or phrases to search for in the submitted audio and replace no keywords An array keywords to which the model should pay particular attention to boosting or suppressing to help it understand context no tag A tag to associate with the request.  Tags appear in usage reports no utteranceEndMs (added in 08.5) a number of milliseconds of silence that deepgram will wait after the last word was spoken before returning an UtteranceEnd event, which is used by jambonz to trigger the transcript webhook if this proprety is supplied.  This is essentially Deepgram\\'s version of continous ASR (and in fact if you enable continuos ASR on Deepgram it will work by enabling this property) no shortUtterance (added in 08.5) Causes a transcript to be returned as soon as the Deepgram is_final property is set.  This should only be used in scenarios where you are expecting a very short confirmation or directed command and you want minimal latency no smartFormatting (added in 08.5) Indicates whether to enable Deepgram\\'s Smart Formatting feature. no\\n\\nibmOptions\\n\\nibmOptions is an object with the following properties. Please refer to the IBM Watson Documentation for detailed descriptions. This option is available in jambonz 0.8.0 or above.\\n\\noption description required sttApiKey IBM api key to authenticate with (overrides setting in jambonz portal) no sttRegion IBM region (overrides setting in jambonz portal) no instanceId IBM speech instance id (overrides setting in jambonz portal) no model The model to use for speech recognition no languageCustomizationId Id of a custom language model no acousticCustomizationId Id of a custom acoustic model no baseModelVersion Base model to be used no watsonMetadata a tag value to apply to the request data provided no watsonLearningOptOut set to true to prevent IBM from using your api request data to improve their service no\\n\\nnvidiaOptions\\n\\nnvidiaOptions is an object with the following properties. Please refer to the Nvidia Riva Documentation for detailed descriptions. This option is available in jambonz 0.8.0 or above.\\n\\noption description required rivaUri grcp endpoint (ip:port) that Nvidia Riva is listening on no maxAlternatives number of alternatives to return no profanityFilter Indicates whether to remove profanity from the transcript no punctuation Indicates whether to provide puncutation in the transcripts no wordTimeOffsets indicates whether to provide word-level detail no verbatimTranscripts Indicates whether to provide verbatim transcripts no customConfiguration An object of key-value pairs that can be sent to Nvidia for custom configuration no\\n\\nsonioxOptions\\n\\nsonioxOptions is an object with the following properties. Please refer to the Soniox Documentation for detailed descriptions. This option is available in jambonz 0.8.2 or above.\\n\\noption description required api_key Soniox api key no model Soniox model to use no (default: precision_ivr) profanityFilter Indicates whether to remove profanity from the transcript no storage properties that dictate whether to audio and/or transcripts.  Can be useful for debugging purposes. no storage.id storage identifier no storage.title storage title no storage.disableStoreAudio if true do not store audio no (default: false) storage.disableStoreTranscript if true do not store transcript no (default: false) storage.disableSearch if true do not allow search no (default: false)', metadata={'source': '/Users/dhorton/beachdog-enterprises/beachdog-networks/git/jambones.org/git/hosted-cpaas/next-static-site/markdown/docs/webhooks/recognizer.md'}), Document(page_content='say\\n\\nThe say command is used to send synthesized speech to the remote party. The text provided may be either plain text or may use SSML tags.  jambonz supports a large number of speech vendors out of the box (see list below), and you may add others via the custom speech API.\\n\\njson\\n{\\n  \"verb\": \"say\",\\n  \"text\": \"hi there!\",\\n  \"synthesizer\" : {\\n    \"vendor\": \"google\",\\n    \"language\": \"en-US\"\\n  }\\n}\\n\\nYou can use the following options in the say action:\\n\\noption description required text text to speak; may contain SSML tags yes synthesizer.vendor speech vendor to use (see list below, along with any others you add via the custom speech API ) no synthesizer.language language code to use. no synthesizer.gender (Google only) MALE, FEMALE, or NEUTRAL. no synthesizer.voice voice to use.  Note that the voice list differs whether you are using aws or Google. Defaults to application setting, if provided. no loop the number of times a text is to be repeated; 0 means repeat forever.  Defaults to 1. no earlyMedia if true and the call has not yet been answered, play the audio without answering call.  Defaults to false no\\n\\nText-to-speech vendors\\n\\njambonz natively supports the following text-to-speech services:\\n- aws\\n- azure\\n- deepgram\\n- elevenlabs\\n- google\\n- ibm\\n- nuance\\n- nvidia\\n- wellsaid\\n- whisper\\n\\nPrev: redirect\\nNext: sip:decline', metadata={'source': '/Users/dhorton/beachdog-enterprises/beachdog-networks/git/jambones.org/git/hosted-cpaas/next-static-site/markdown/docs/webhooks/say.md'}), Document(page_content='Webhook API\\n\\nNote: this page describes how to build applications using webhooks.  If you prefer to use the websocket API, please visit this page.\\n\\nTLDR;\\n- Use npx create-jambonz-app to scaffold a webhook application\\n- See @jambonz/node-client for Node.js API\\n\\nOr:\\n- use Node-RED and install @jambonz/node-red-contrib-jambonz\\n\\nOverview\\n\\njambonz controls calls through the use of JSON payloads that are exchanged either over an HTTP(s) or a websocket connection.  When an incoming call for your account is received, jambonz retrieves the URL that you have configured for the application you want to run.  If the URL begins with \\'http(s)://\\' jambonz makes an http request to the URL, while if the URL starts with \\'ws(s)://\\' jambonz establishes a websocket connection to that URL. jambonz then sends an initial message describing the incoming call, and your webapp is then responsible for returning a JSON payload that indicates how you want the call handled.\\n\\nEither way (http or websocket) the details of the JSON payloads are the same.  The information below pertains to using HTTP connections; for information describing the websocket interface see here.\\n\\nWhen an incoming call for your account is received, jambonz makes an HTTP request to a URL that you have configured and your webapp will then return a response containing a JSON body that indicates how you want the call handled.\\n\\nYou can develop your webapp using whatever language or framework you like, but the quickest way to scaffold up a webapp is by using our Node.js framework\\n\\nIf you want to generate an outbound call it works similarly: you will make an HTTP request using the REST API and in it you will specify a URL or application identifier that will be invoked once the call is answered.  Once again, your response to that HTTP request will contain a JSON body that indicates how you want the call handled.\\n\\nSimple enough, right?\\n\\nBasic JSON message structure\\n\\nThe JSON payload that you provide in response to an HTTP request must be an array with each item describing a task that the platform shall perform.  These tasks are executed sequentially in the order they appear in the array.  Each task is identified by a verb (e.g. \"dial\", \"gather\", \"hangup\" etc) with associated detail to configure how the action should be carried out.\\n\\nIf the caller hangs up during the execution of an application for that call, the current task is allowed to complete and any remaining tasks in the application are ignored.\\n\\njson\\n[\\n  {\\n    \"verb\": \"say\",\\n    \"text\": \"Hi there!  Please leave a message at the tone.\",\\n    \"synthesizer\": {\\n      \"vendor\": \"Google\",\\n      \"language\": \"en-US\",\\n      \"gender\": \"FEMALE\"\\n    }\\n  },\\n  {\\n    /* ..next verb */\\n  }\\n]\\n\\nSome verbs allow other verbs to be nested; e.g. \"gather\" can have a nested \"say\" command in order to play a prompt and collect a response in one command:\\n\\njson\\n{\\n  \"verb\": \"gather\",\\n  \"actionHook\": \"/gatherCardNumber\",\\n  \"input\": [\"speech\", \"dtmf\"],\\n  \"timeout\": 16,\\n  \"numDigits\": 6,\\n  \"recognizer\": {\\n    \"vendor\": \"Google\",\\n    \"language\": \"en-US\"\\n  },\\n  \"say\": {\\n    \"text\": \"Please say or enter your six digit card number now\",\\n    \"synthesizer\": {\\n      \"vendor\": \"Google\",\\n      \"language\": \"en-US\",\\n      \"gender\": \"FEMALE\"\\n    }\\n  }\\n}\\n\\nAltogether then, a simple voicemail application could look like this:\\njson\\n[\\n  {\\n    \"verb\": \"say\",\\n    \"text\": \"Hi there!  Please leave a message at the tone and we will get back to you shortly.\"\\n  },\\n  {\\n    \"verb\": \"listen\",\\n    \"actionHook\": \"http://example.com/voicemail\",\\n    \"url\": \"wss://example.com/my-recorder\",\\n    \"finishOnKey\": \"#\",\\n    \"metadata\": {\\n      \"topic\": \"voicemail\"\\n    },\\n    \"playBeep\": true,\\n    \"timeout\": 20\\n  },\\n  {\\n    \"verb\": \"say\",\\n    \"text\": \"Thanks for your message.  We\\'ll get back to you\"\\n  }\\n]\\n\\nHTTP connection details\\n\\nEach HTTP request that jambonz makes to one of your callbacks will include (at least) the following information either as query arguments (in a GET request) or in the body of the response as a JSON payload (in a POST request):\\n\\ncall_sid: a unique identifier for the call.\\n\\napplication_sid: a unique identifier for the jambonz application controlling this call\\n\\naccount_sid: a unique identifier for the jambonz account associated with the application\\n\\ndirection: the direction of the call: inbound or outbound\\n\\nfrom: the calling party number\\n\\nto: the called party number\\n\\ncaller_id: the caller name, if known\\n\\ncall_status: current status of the call, see table below\\n\\nsip_status: the most recent sip status code received or generated for the call\\n\\nAdditionally, the request MAY include\\n\\nparent_call_sid: the call_sid of a parent call to this call, if this call is a child call\\n\\nAnd the initial webhook for a new incoming call will have:\\n\\noriginating_sip_trunk_name: name of the SIP trunk that originated the call to the platform\\n\\noriginating_sip_ip: the ip address and port of the sip gateway that originated the call\\n\\nFinally, if you specify to use a POST method for the initial webhook for an incoming call, the JSON payload in that POST will also contain the entire incoming SIP INVITE request details in a \\'sip\\' property (this is not provided if a GET request is used).  This can be useful if you need a detailed look at all of the SIP headers, or the Session Description Protocol being offered.\\n\\nNote also that you can add arbitrary information of your own into the payloads that jambonz sends you by using the tag verb early in your application flow.  Data elements that you provide in that verb will then come back to you in further webhook callbacks for that call.  This can be useful for managing stateful information during a call that you may want to drive decision logic later in the call.\\n\\ncall_status value description trying a new incoming call has arrived or an outbound call has just been sent ringing a 180 Ringing response has been sent or received early-media an early media connection has been established prior to answering the call (183 Session Progress) in-progress call has been answered completed an answered call has ended failed a call attempt failed busy a call attempt failed because the called party returned a busy status no-answer a call attempt failed because it was not answered in time\\n\\nSecuring your HTTP Endpoints\\n\\nBefore we go any further, let\\'s talk about how to properly secure your endpoints.\\n\\nThis is important because your response to HTTP webhook requests will contain information that must be kept private between you and the jambonz platform. We recommend that you use HTTPS connections secured with TLS certificates for your endpoints, and that you additionally takes steps to verify that the incoming request was actually sent by jambonz, and not an imposter.\\n\\nFor the latter, you have two options:\\n- You can use HTTP basic authentication to secure your endpoint with a username and password.\\n- On the hosted platform, you can verify the signature of the HTTP request to know that it was sent by jambonz.\\n\\nVerifying a signed request\\n\\nThe HTTP requests sent to you from the hosted platform will include a Jambonz-Signature header, which is a hash of the request payload signed with your webhook secret, which you can view (and when desired, change) in the self-service portal.  Using that secret, you can verify that the request was actually sent by jambonz.\\n\\nWhen using the Node.js SDK, this is done simply as http middleware.\\n```js\\nconst express = require(\\'express\\');\\nconst app = express();\\nconst {WebhookResponse} = require(\\'@jambonz/node-client\\');\\n\\napp.use(WebhookResponse.verifyJambonzSignature(\\'\\'));\\napp.use(\\'/\\', routes); / only requests with valid signatures will get here /\\n```\\n\\nInitial state of incoming calls\\n\\nWhen the jambonz platform receives a new incoming call, it responds 100 Trying to the INVITE but does not automatically answer the call.  It is up to your application to decide how to finally respond to the INVITE.  You have some choices here.\\n\\nYour application can:\\n\\nanswer the call, which connects the call to a media endpoint that can perform IVR functions on the call,\\n\\noutdial a new call, and bridge the two calls together (i.e use the dial verb),\\n\\nreject the call, with a specified SIP status code and reason,\\n\\nestablish an early media connection and play audio to the caller without answering the call.\\n\\nThe last is interesting and worthy of further comment.  The intent is to let you play audio to callers without necessarily answering the call.  You signal this by including an \"earlyMedia\" property with a value of true in the application.  When receiving this, jambonz will create an early media connection (using 183 Session Progress), as shown in the example below.\\n\\nNote: an early media connection will not be possible if the call has already been answered by an earlier verb in the application.  In such a scenario, the earlyMedia property is ignored.\\n\\njson\\n[\\n  {\\n    \"verb\": \"say\",\\n    \"earlyMedia\": true,\\n    \"text\": \"Please call back later, we are currently at lunch\"\\n    \"synthesizer\": {\\n      \"vendor\": \"aws\",\\n      \"language\": \"en-US\",\\n      \"voice\": \"Amy\"\\n    },\\n    {\\n      \"verb\": \"sip:decline\",\\n      \"status\": 480,\\n      \"headers\": {\\n        \"Retry-After\": 1800\\n      }\\n    }\\n  }\\n]\\nPlease note:\\n- The say, play, gather, listen, and transcribe verbs all support the \"earlyMedia\" property.\\n\\nThe dial verb supports a similar feature of not answering the inbound call unless/until the dialed call is answered via the \"answerOnBridge\" property.\\n\\nAuthenticating SIP clients\\n\\njambonz allows SIP clients such as softphones, SIP phones, and webrtc clients to register with the platform and make and receive calls.\\n\\nManaging sip registrations is a shared activity between the platform and your application, and uses webhooks.  The platform handles the sip messaging details, but the determination of whether to authenticate a specific sip user is the responsibility of the application, which is notified of incoming REGISTER or INVITE requests by means of a registration webhook.\\n\\nThis approach ensures that sip credentials - which embody highly confidential and private information - are stored within customer networks and never directly exposed to the jambonz platform.\\n\\nWhen the platform receives an incoming sip request from an endpoint that is not a carrier SIP trunk, the request is challenged with a 401 Unauthorized response that includes a WWW-Authenticate header.\\n\\nWhen the originating sip device then resends the request with credentials (e.g. an Authorization header) the sip domain is retrieve from the request and used to lookup the account that owns that domain.  Then, the associated registration webhook is invoked with the details provided in the Authorization header, e.g.:\\n\\njson\\n{\\n  \"method\": \"REGISTER\",\\n  \"realm\": \"example.com\",\\n  \"username\": \"foo\",\\n  \"expires\": 3600,\\n  \"nonce\": \"InFriVGWVoKeCckYrTx7wg==\",\\n  \"uri\": \"sip:example.com\",\\n  \"algorithm\": \"MD5\",\\n  \"qop\": \"auth\",\\n  \"cnonce\": \"03d8d2aafd5a975f2b07dc90fe5f4100\",\\n  \"nc\": \"00000001\",\\n  \"response\": \"db7b7dbec7edc0c427c1708031f67cc6\"\\n}\\nThe application\\'s responsibility is to retrieve the password associated with the username, and perform digest authentication to authenticate the request using the information provided, including the calculated response value.\\n\\nRegardless of whether the request is authenticated or not, the application should respond with a 200 OK to the http POST and with a JSON body.\\n\\nThe JSON body in the response if the request is authenticated should simply contain a status attribute with a value of ok, e.g.:\\njson\\n{\\n  \"status\": \"ok\"\\n}\\n\\nIf the application wishes to enforce a shorter expires value, it may include that value in the response, e.g.:\\njson\\n{\\n  \"status\": \"ok\",\\n  \"expires\": 1800\\n}\\n\\nThe JSON body in the response if the request is not authentication should contain a status of fail, and optionally a msg attribute, e.g.\\njson\\n{\\n  \"status\": \"fail\",\\n  \"msg\" : \"invalid password\"\\n}\\n\\nSpeech integration\\n\\nThe platform makes use of text-to-speech as well as real-time speech recognition.  Both Google and AWS are supported for text to speech (TTS) as well as speech to text (STT).\\n\\nSynthesized audio is cached for up to 24 hours, so that if the same {text, language, voice} combination is requested more than once in that period it will be served from cache, thus reducing your TTS bill from from Google or AWS.\\n\\nWhen you configure your applications in the portal, you can set defaults for the language and voice to use for speech synthesis as well as the language to use for speech recognition.  These can then be overridden by verbs in the application, by using the \\'synthesizer\\' and \\'recognizer\\' properties./\\n\\nWebhook URL specifiers\\n\\nMany of the verbs specify a webhook that will be called when the verb completes or has some information to deliver to your application.  These verbs contain a property that allow you to configure that webhook.  By convention, the property name will always end in \"Hook\"; e.g \"actionHook\", \"dtmfHook\", and so on.\\n\\nYou can either specify the webhook as a simple string specifying either an absolute or relative url:\\n\\njson\\n\"actionHook\": \"https://my.appserver.com/results\"\\njson\\n\"actionHook\": \"/results\"\\nIn the latter case, the base url of the application will be applied.\\n\\nAlternatively, you can provide an object containing a url and optional method and basic authentication parameters, e.g.:\\n\\njson\\n\"actionHook\": {\\n  \"url\": \"https://my.appserver.com/results\",\\n  \"method\": \"GET\",\\n  \"username\": \"foo\",\\n  \"password\": \"bar\"\\n}\\nIn the sections that follow, we will describe each of the verbs in detail.\\n\\nNext: conference', metadata={'source': '/Users/dhorton/beachdog-enterprises/beachdog-networks/git/jambones.org/git/hosted-cpaas/next-static-site/markdown/docs/webhooks/overview.md'}), Document(page_content='listen\\n\\njambonz does not have a \\'record\\' verb. This is by design, for data privacy reasons:\\n\\nRecordings can contain sensitive and confidential information about your customers, and such data is never stored at rest in the jambonz core.\\n\\nInstead, jambonz provides the listen verb, where an audio stream(s) can be forked and sent in real-time to your application for processing.\\n\\nThe listen verb can also be nested in a dial or config verb, which allows the audio for a call between two parties to be sent to a remote websocket server.\\n\\nTo utilize the listen verb, the customer must implement a websocket server to receive and process the audio.  The endpoint should be prepared to accept websocket connections with a subprotocol name of \\'audio.jambonz.org\\'.\\n\\nThe format of the audio data sent over the websocket is 16-bit PCM encoding, with a user-specified sample rate.  The audio is sent in binary frames over the websocket connection.\\n\\nAdditionally, one text frame is sent immediately after the websocket connection is established.  This text frame contains a JSON string with all of the call attributes normally sent on an HTTP request (e.g. callSid, etc), plus sampleRate and mixType properties describing the audio sample rate and stream(s).  Additional metadata can also be added to this payload using the metadata property as described in the table below.  Once the intial text frame containing the metadata has been sent, the remote side should expect to receive only binary frames, containing audio.\\n\\nNote that the remote side can optionally send messages and audio back over the websocket connection, as described below in Birectional Audio.\\n\\njson\\n{\\n  \"verb\": \"listen\",\\n  \"url\": \"wss://myrecorder.example.com/calls\",\\n  \"mixType\" : \"stereo\"\\n}\\n\\nYou can use the following options in the listen action:\\n\\noption description required actionHook webhook to invoke when listen operation ends.  The information will include the duration of the audio stream, and also a \\'digits\\' property if the recording was terminated by a dtmf key. yes finishOnKey The set of digits that can end the listen action no maxLength the maximum length of the listened audio stream, in secs no metadata arbitrary data to add to the JSON payload sent to the remote server when websocket connection is first connected no mixType \"mono\" (send single channel), \"stereo\" (send dual channel of both calls in a bridge), or \"mixed\" (send audio from both calls in a bridge in a single mixed audio stream) Default: mono no passDtmf if true, any dtmf digits detected from the caller will be passed over the websocket as text frames in JSON format.  Default: false no playBeep true, false whether to play a beep at the start of the listen operation.  Default: false no sampleRate sample rate of audio to send (allowable values: 8000, 16000, 24000, 48000, or 64000).  Default: 8000 no timeout the number of seconds of silence that terminates the listen operation. no transcribe a nested transcribe verb no url url of remote server to connect to yes wsAuth.username HTTP basic auth username to use on websocket connection no wsAuth.password HTTP basic auth password to use on websocket connection no\\n\\nPassing DTMF\\n\\nAny DTMF digits entered by the far end party on the call can optionally be passed to the websocket server as JSON text frames by setting the passDtmf property to true. Each DTMF entry is reported separately in a payload that contains the specific DTMF key that was entered, as well as the duration which is reported in RTP timestamp units.  The payload that is sent will look like this:\\n\\njson\\n{\\n  \"event\": \"dtmf\",\\n  \"dtmf\": \"2\",\\n  \"duration\": \"1600\"\\n}\\n\\nBidirectional audio\\n\\nAudio can also be sent back over the websocket to jambonz.  This audio, if supplied, will be played out to the caller.  (Note: Bidirectional audio is not supported when the listen is nested in the context of a dial verb).\\n\\nThe far-end websocket server supplies bidirectional audio by sending a JSON text frame over the websocket connection:\\njson\\n{\\n  \"type\": \"playAudio\",\\n  \"data\": {\\n    \"audioContent\": \"base64-encoded content..\",\\n    \"audioContentType\": \"raw\",\\n    \"sampleRate\": \"16000\"\\n  }\\n}\\nIn the example above, raw (headerless) audio is sent.  The audio must be 16-bit pcm encoded audio, with a configurable sample rate of either 8000, 16000, 24000, 32000, 48000, or 64000 khz.  Alternatively, a wave file format can be supplied by using type \"wav\" (or \"wave\"), and in this case no sampleRate property is needed.  In all cases, the audio must be base64 encoded when sent over the socket.\\n\\nIf multiple playAudio commands are sent before the first has finished playing they will be queued and played in order. You may have up to 10 queued playAudio commands at any time.\\n\\nPrev: lex\\nNext: message', metadata={'source': '/Users/dhorton/beachdog-enterprises/beachdog-networks/git/jambones.org/git/hosted-cpaas/next-static-site/markdown/docs/webhooks/listen.md'})]}\n"
     ]
    }
   ],
   "source": [
    "# Run a query\n",
    "query = \"Does jambonz support IBM as a speech recognizer?  Answer only from the context documents provided\"\n",
    "result = qa({\"query\": query})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
